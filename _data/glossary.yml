# A
- name: "algorithm"
  wiki: "Algorithm"
  definition: "is a set of rules in problem-solving operations. Creating algorithms is a foundation of programming, where a developer defines a finite sequence of well-defined instructions to perform computations and process data. Among the typical elements of an algorithm, regardless of programming language, are conditionals and loops that enable repetitive actions and logical decisions."
- name: "artificial-intelligence"
  wiki: "Artificial_intelligence"
  definition: "is a branch of Computer Science dealing with cognitive technology and simulation of intelligent behavior, including planning, learning, reasoning, problem-solving, knowledge representation, perception, motion, and manipulation. AI systems can range from simple rule-based systems to complex machine learning models that improve over time by learning from data."
# B
- name: "bash"
  wiki: "Bash_(Unix_shell)"
  definition: "is a command language in the Unix shell that allows users to execute various processes by writing text commands in the terminal window. While the Unix shell is a general interface for command execution, BASH (Bourne Again SHell) is a specific implementation that offers enhanced features like scripting capabilities, improved command-line editing and customizable user environments."
- name: ".bashrc"
  wiki: "Bash_(Unix_shell)#Startup_scripts"
  definition: "is a script file executed whenever a new terminal session is started in an interactive shell environment for the Bash shell. It is used to configure the user's shell environment, including setting environment variables, defining aliases, customizing the command prompt and running startup commands. The `.bashrc` file is typically located in the user's home directory and allows users to personalize their command-line interface, automate tasks and enhance their productivity by tailoring the shell's behavior to their preferences."
- name: "bash-scripting"
  wiki: "Shell_script"
  definition: "is the process of writing scripts using the BASH language to automate tasks in the Unix shell. It allows users to combine multiple commands into a single script file and execute repetitive tasks efficiently. Bash scripting allows users to simplify or automate operations performed on text or multiple files, including numerical calculations, saving time and reducing the potential for errors by eliminating the need to do it manually in the GUI or enter each command every time you need it."
- name: "big-data"
  wiki: "Big_data"
  definition: "focuses on the large size of data, its variety, and the velocity of generating and processing. These parameters continually expand and become a bottleneck on existing computational approaches. It also integrates modern (i) analytical techniques (e.g., machine learning), (ii) technologies (e.g., distributed computing), and (iii) visualization solutions (e.g., interactive graphing and infographics), applied during the life cycle of Big Data."
- name: "binary"
  wiki: "Binary_code"
  definition: "is a numbering system where data is expressed in base-2, using only two symbols, typically 0 and 1, to represent information. It is the fundamental language of computers, where each binary digit (bit) corresponds to a power of 2. Binary is used to encode all types of data, including numbers, text and instructions, enabling digital devices to perform calculations, store information and execute commands. This system underpins all computer operations and is essential for data processing, memory storage and communication in digital electronics."
# C
- name: "code"
  wiki: "Source_code"
  definition: "in programming, is a set of instructions written in programming languages that a computer can execute. Source code forms the foundation of software applications, scripts and systems, enabling them to perform specific tasks and processes. It can range from simple scripts that automate routine tasks to complex systems that power applications and services. Writing source code involves using programming languages like Python, Java, C++ and many others, each suited to different types of development. Source code is essential for creating, maintaining and updating software, and it must be compiled or interpreted to run on a computer."
- name: "code-development"
  wiki: "Coding_best_practices"
  definition: "is the process of writing, testing, and maintaining the source code of both software applications and smaller code pieces that facilitate researchers' tasks. It involves designing algorithms, implementing functionality, and debugging to ensure the software operates correctly. Key aspects include following coding best practices such as writing clean and readable code, using meaningful variable names, commenting and documenting code, maintaining version control, and performing regular code reviews. Adhering to these practices improves code quality, facilitates collaboration, enhances maintainability, and ensures that the software is reliable and scalable."
- name: "collaboration-tools"
  wiki: "List_of_collaborative_software"
  definition: "are software applications designed to facilitate teamwork and communication among individuals or groups, especially in a remote or distributed environment. These tools include platforms for real-time messaging and chat (e.g., Slack, Microsoft Teams), video conferencing (e.g., Zoom, Google Meet), project management (e.g., Trello, Asana) and document sharing and co-editing (e.g., Google Drive, Dropbox). Collaboration tools enhance productivity by enabling seamless communication, file sharing and task coordination, ensuring that team members can work together efficiently, regardless of their physical locations. They often feature integration with other software and services, further streamlining workflows and improving project management."
- name: "command-line"
  wiki: "Command-line_interface"
  definition: "is a text interface for the computer that passes the predefined commands to the operating system. Commands trigger the execution of various processes. These commands trigger the execution of various processes, allowing users to perform tasks such as file manipulation, program execution and system management directly through text input in the terminal window."
- name: "computing-tools"
  wiki: "Computing"
  definition: "*(in this workbook)* refer to the resources and technologies that support computer-powered computations in research. This includes High-Performance Computing (HPC), cloud computing, specialized computer software, high-speed networking, and programming tools. These tools facilitate efficient and effective data processing, analysis and computation, enabling researchers to handle complex and large-scale tasks with enhanced speed and precision."
- name: "containers"
  wiki: "Containerization_(computing)"
  definition: "are executable packages that bundle a specific application code along with its dependencies, ensuring that the software can run consistently across different environments. Containers include all necessary elements, such as libraries, system tools and runtime, needed to execute the application. Popular containerization platforms include Docker, commonly used for local machines, and Apptainer (formerly Singularity), designed for High-Performance Computing (HPC) environments. These tools enable the creation of portable and reproducible software environments, facilitating development, testing and deployment processes by isolating applications from underlying system variations."
# D
- name: "data-acquisition"
  wiki: "Data_acquisition"
  definition: "is the process of collecting and measuring information from various sources to be used for analysis and research. This involves gathering data through sensors, instruments, surveys and other methods, and often includes digitizing and storing the data for further processing. In scientific research and engineering, data acquisition systems (DAQ) capture physical phenomena like temperature, pressure and sound, converting them into digital signals for analysis. Effective data acquisition ensures the accuracy and reliability of the data, forming the foundation for insightful analysis and informed decision-making."
- name: "data-management"
  wiki: "Data_management"
  definition: "is the process of handling, storing, and organizing data throughout its lifecycle to ensure its accuracy, accessibility and reliability. It includes tasks such as data collection, storage, processing, maintenance and archiving. Effective data management involves implementing policies and practices for data governance, quality control and security to protect data integrity and privacy. In research and business, good data management practices facilitate efficient data retrieval and analysis, support decision-making and enhance the overall value and usability of data."
- name: "data-manipulation"
  wiki: "Data_processing"
  definition: "is the process of adjusting, organizing, and transforming data to make it suitable for analysis. In the context of data processing, data preparation, or data wrangling, especially in research, it involves tasks like cleaning data, merging datasets, and converting data formats to ensure accuracy and consistency for subsequent analysis. It should not be confused with <a href='https://en.wikipedia.org/wiki/Misuse_of_statistics#Data_manipulation' target='_blank'>intentional data manipulation</a> in statistical analyses, which is a serious issue that undermines the transparency and honesty of research."
- name: "data-preview"
  wiki: ""
  definition: "is the process of viewing data without downloading it, particularly useful for data stored remotely on HPC systems. This can be achieved through various methods such as using command-line tools like `less`, `more` and `head` to quickly view text files, mounting folders with `sshfs` to access remote files as if they were local and utilizing X11-forwarding for running graphical applications remotely with local display. Additionally, Open OnDemand (OOD) provides a web-based interface for browsing and previewing files, while tools like Jupyter notebooks facilitate remote data visualization directly in the browser. These approaches enhance efficiency by allowing users to inspect data quickly and conveniently without transferring large files."
- name: "data-science"
  wiki: "Data_science"
  definition: "is a modern conception of efficient computational processing of large sets of digital information for data mining and knowledge discovery. Data Science focuses on solving various technical challenges related to Big Data and developing innovative techniques unique to digital data (e.g., Machine Learning). It is a highly interdisciplinary field using the latest developments in Computer & Information Science, also strongly supported by Mathematics and Statistics, and complemented by specific Domain Knowledge."
- name: "data-storage"
  wiki: "Computer_data_storage"
  definition: "is the method of saving digital information in a secure and organized manner for future access and use. It encompasses various technologies and solutions, including physical devices like hard drives and SSDs, cloud-based services such as Amazon S3 and Google Cloud Storage and specialized HPC storage solutions. In HPC environments, long-term data storage is essential for backup and archiving, ensuring that valuable research data is preserved over time. Additionally, online databases and data repositories (e.g., SQL databases, data warehouses and repositories like Zenodo) facilitate the efficient storage, retrieval, and sharing of data. Effective data storage practices include regular backups, data encryption, and implementing redundancy to prevent data loss, which are critical for preserving data integrity and supporting data analysis."
- name: "data-transfer"
  wiki: "Data_communication"
  definition: "is the process of moving data from one location to another, which can be between different devices, systems or network locations. In the context of HPC and research, data transfer involves using secure and efficient methods to handle large datasets. Tools such as SCP (Secure Copy Protocol), SFTP (Secure File Transfer Protocol) and Globus facilitate data transfer, ensuring integrity and speed while minimizing the risk of data loss or corruption. Efficient data transfer is crucial for collaborative research, backup and data analysis."
- name: "data-types"
  wiki: ""
  definition: "refer to the various forms in which data can exist and be utilized across different contexts. In classical data types, we have structured data (organized in predefined formats like databases and spreadsheets) and unstructured data (lacking specific structure, such as text documents and images). In programming, data types include primitive types (integers, floats, booleans), composite types (arrays, lists, tuples), objects, data frames (e.g., pandas DataFrame in Python), arrays (e.g., NumPy arrays) and matrices. Additionally, data types encompass various file formats like text files (TXT, CSV), binary files (EXE, BIN), markup languages (XML, HTML) and specialized formats (HDF5, JSON) for specific data storage and exchange needs."
- name: "data-wrangling"
  wiki: "Data_wrangling"
  definition: "is the process of cleaning, transforming and organizing raw data into a usable format that meets the specific requirements of a project. This involves various steps such as removing inconsistencies, handling missing values, normalizing data structures, and converting data types. Data wrangling is essential for preparing data for analysis, ensuring accuracy and enhancing the quality of insights derived from the data. It is a critical step in data science, enabling researchers and analysts to work with reliable and well-structured data."
- name: "developer-tools"
  wiki: "Category:Programming_tools"
  definition: "are software applications and utilities that assist programmers in creating, debugging, maintaining and optimizing code. These tools include integrated development environments (IDEs) like Visual Studio Code and PyCharm, notebook interfaces like Jupyter and version control systems like Git. They support various programming languages and libraries, providing features like code completion, syntax highlighting, and real-time error detection. Developer tools also encompass build automation tools like Maven and Gradle, debugging tools, performance profilers and repository hosting platforms like GitHub and GitLab. These tools enhance productivity, facilitate collaboration, streamline workflows and ensure that the development process is efficient and effective."
- name: "development-environment"
  wiki: "Deployment_environment#Development"
  definition: "is a workspace for developers where they create and modify the source code of their application (software, web, etc.). Nowadays, professional developers usually use an Integrated Development Environment (IDE) that is a software suite partially with a graphical interface to make various things easier for the programmer (general code management, tracking and pushing changes, file system browsing, file preview & editing, kernel loading, autocomplete, formatting, etc.); programming environment, is a layer of settings specific for a given programming language or type of developed application. It can be isolated from the general operating system and provides a kind of virtual environment with adjusted software configuration or modules loaded in a selected release. Virtual environments are commonly used when programming in Python and can be easily created using Conda ( environment management system)."
- name: "digital-data"
  wiki: "Digital_data"
  definition: "is a collection of information represented in a computer-readable format. Each piece of data or datum, is stored as a binary value, where it can be either '0' (false) or '1' (true), representing one bit of information. Digital data encompasses various forms of information, including text, images, audio and video, all encoded in binary. This format allows data to be easily processed, stored and transmitted by digital devices. The use of digital data is fundamental to computing and underpins modern technologies and applications across numerous fields."
- name: "distributed-computing"
  wiki: "Distributed_computing"
  definition: "is a system of multiple computer machines connected over a network to create a computing cluster that appears as a single computer to the end-user. It provides a unified environment with access to shared resources (software, data, storage) and coordinated computing power. Distributed computing is a technique typically used in the High-Performance Computing (HPC) infrastructures."
- name: "documentation"
  wiki: "Documentation"
  definition: "is the process of creating and maintaining written records that explain how a project, system, or piece of software works. In research and software development, documentation includes user guides, technical manuals, code comments, and project reports that provide essential information for understanding, using, and maintaining the work. Good documentation ensures knowledge retention, making it easier for others to learn, reproduce, and build upon the work, and it facilitates collaboration and consistency across teams."
# E
# F
- name: "file-system"
  wiki: "File_system"
  definition: "is the organization of data retained in the digital storage on the computing machine. The content consists of hierarchically ordered folders and files. Folders are user-categorized containers for files. Files contain data and consume digital storage space. Some files belong to the operating system and include configurations, source code, and executables of various programs. Each file and folder is assigned an absolute path that defines its location in the file system. Knowing this path is very useful for navigating the file system from the command line."
# G
- name: "gnuplot"
  wiki: "Gnuplot"
  definition: "is a free program that can generate two- and three-dimensional plots of functions, data, and data fits. It works as both command-line and GUI interface. You can export graphic files in a variety of formats (SVG, PNG, etc.) or analyze the results interactively, customizing the graph as you wish."
- name: "graphic-tools"
  wiki: "Graphics"
  definition: "refer to visual representations of data, images and designs created and manipulated using computer software. In the context of computing and research, graphics involve generating plots, charts and visualizations to represent complex data sets clearly and intuitively. Tools and libraries such as Matplotlib, Plotly, and ggplot2 in programming languages like Python and R are commonly used for creating these visualizations. Graphics also encompass design and editing software like Adobe Photoshop and Illustrator, which are used for creating and modifying images and illustrations. Effective use of graphics enhances data interpretation, communication and overall presentation quality in various fields."
- name: "GUI"
  wiki: "Graphical_user_interface"
  definition: "short for Graphical User Interface, is an interface through which users interact with computers and other electronic devices using icons, buttons and other visual elements. Unlike command-line interfaces, GUIs provide a more intuitive and user-friendly experience by allowing users to navigate and control software applications through graphical representations rather than text commands. GUIs are prevalent in operating systems (like Windows, macOS and Linux), software applications, and mobile devices, making technology accessible to a broader audience by simplifying complex operations through visual cues and interactive elements."
# H
- name: "hpc"
  wiki: "High-performance_computing"
  definition: "is the practice of performing computations that require more power than a single computer can provide. HPC utilizes dedicated infrastructure within the framework of distributed computing, combining the power of multiple computers through networks such as computer clusters, supercomputers and cloud-based services. This aggregation allows for the efficient processing of complex tasks that demand significant computational resources."
- name: "hpc-cluster"
  wiki: "Computer_cluster"
  definition: "or a computer cluster is a group of computing machines or servers that work together as a single system. Clusters provide high performance computing and parallel processing by distribution of tasks across multiple machines."

- name: "HTML"
  wiki: "HTML"
  definition: "HyperText Markup Language is the standard language for webpages. HTML is used to display the contents of a website on a web browser."
# I
- name: "IDE"
  wiki: "Integrated_development_environment"
  definition: "is an application for software development that includes a code editor, debugging tools, and version control system. IDE softwares are designed to make the process of writing, testing, and debugging code easier and more efficient."
- name: "in-browser"
  wiki: "Web_browser"
  definition: "is an application used to access the internet and browse websites and web pages. It interprets and displays HTML, CSS and JavaScript code, allowing users to interact with online content and services. Modern web browsers, such as Chrome, Firefox and Safari, support a wide range of in-browser services, including web-based applications like Open OnDemand (OOD) and JupyterLab. These services enable users to access HPC resources, run computational notebooks, manage files and perform complex tasks directly from the browser, providing a flexible and powerful interface for both everyday browsing and specialized computing needs."
- name: "information"
  wiki: "Information"
  definition: "is a meaningful and organized product of data processing. It maintains data compression, encapsulates densification of value and veracity, and provides context for querying in the analysis."
- name: "interactive-graphing"
  wiki: "Category:Plotting_software"
  definition: "is a method for data visualization that enables users to interact with the data on-the-fly, see the details such as numerical values, and freely customize the final plot. That is a modern approach that gives greater insight into the dataset and allows for collaborative work on data analysis."
# J
- name: "job-scheduling"
  wiki: "Batch_processing"
  definition: "is the process of managing and allocating computational tasks across shared computing resources such as HPC infrastructure. Since multiple users need to access these resources, job scheduling ensures an equal or prioritized distribution by gathering jobs into a queue for batch processing. These jobs are then sent to different nodes for computation, optimizing memory allocation and computer power usage. Job scheduling systems like SLURM and PBS automate this process, balancing the computational demand with the available processing power to maximize the efficiency and throughput of the HPC infrastructure, minimizing waiting times and ensuring fair access for all users."
- name: "jupyter"
  wiki: "Project_Jupyter"
  definition: "is an integrated development environment (IDE) with an interactive web-based computing interface that supports programming in multiple languages, including Python, Java, R, Julia, Matlab, Octave, etc. The Jupyter interface has a form of a notebook, where you can do it all at once, (i) develop and execute code cells, (ii) write comments and documentation in markdown, and (iii) visualize and analyze results with an interactive graphing."
# K
- name: "kernel"
  wiki: "IPython#Project_Jupyter"
  definition: "in computing is the core component of an operating system, responsible for managing memory, tasks and processes, facilitating communication between hardware and software. In the context of Jupyter, a kernel refers to the computational engine that executes the code contained in Jupyter notebooks. For instance, Python and Julia kernels in Jupyter allow users to write and run code in these respective programming languages, handling the execution of commands and returning the results within the notebook interface. This setup enables interactive computing and supports data analysis, visualization and other tasks within a flexible and user-friendly environment."
- name: "knowledge"
  wiki: "Knowledge"
  definition: "is an extracted non-trivial insight from the data classification and analysis of information. Knowledge, while applied, leads to problem-solving, improvements, and steady development."
# L
- name: "library"
  wiki: "Library_(computing)"
  definition: "in programming is a collection of prewritten code designed to be used for common tasks. Programming libraries provide reusable functions, classes, and routines that developers can integrate into their own programs. For example, Numpy in Python is used for working with large arrays of data, offering a wide range of mathematical and statistical functions. Libraries help developers save time and effort by leveraging existing solutions, ensuring consistency, and improving code reliability."
- name: "library-package-module"
  wiki: "Modular_programming"
  definition: "is a collection of pre-written source code, dependencies, and modules available for use in programming. In the context of HPC clusters, these modules are pre-installed and can be easily loaded to provide essential functions and tools, streamlining development and ensuring compatibility with various software requirements. They help developers save time by reusing code and avoiding the need to build software from scratch."
- name: "linux"
  wiki: "Linux"
  definition: "is a family of operating systems that includes Ubuntu, Debian, Fedora, etc. These are open-source operating systems based on the Linux kernel."
- name: "local-machine"
  wiki: ""
  definition: "is the computer that the user is using with direct access to it. Usually, it is your personal computer."
- name: "loop"
  wiki: "Control_flow#Loops"
  definition: "is a set of code that is written in such a way that it performs a task repetitively until a condition is met."
# M
- name: "machine-learning"
  wiki: "Machine_learning"
  definition: "is a field of study focused on developing advanced computer algorithms that search for deeply coupled patterns in massive, disparate data and enable knowledge extraction. Machine learning methods are trained with large sets of data, and they learn from examples to make intelligent decisions without being explicitly programmed."
- name: "module"
  wiki: "Modular_programming"
  definition: "is a small piece of a larger program. Modular programming is a way to design software such that each module is independent and can be used to execute a part of the function."
# N
- name: "nextflow"
  wiki: "Nextflow"
  definition: "is a domain specific language used for bioinformatics data analysis. It uses software containers and enables reproducible scientific workflows."
- name: "node"
  wiki: "Computer_cluster"
  definition: "is a unit in an HPC cluster. It can be a single computer or a server in a collection that makes the cluster."
# O
- name: "office-tools"
  wiki: "Productivity_software"
  definition: "are productivity software applications designed to assist with common tasks in a professional or academic setting. These tools include word processors (e.g., Microsoft Word, Google Docs), spreadsheets (e.g., Microsoft Excel, Google Sheets), presentation software (e.g., Microsoft PowerPoint, Google Slides), email clients (e.g., Microsoft Outlook, Gmail), password managers and schedule managers among other applications. Office tools facilitate document creation, data analysis, notetaking, presentations and communication, enhancing efficiency and collaboration. They often come with features like templates, real-time collaboration, cloud storage and integration with other software, making them essential for productivity and effective project management."
- name: "OOD"
  wiki: "Open OnDemand"
  definition: "is a web-based interface that provides easy and flexible access to High-Performance Computing (HPC) resources. It allows users to manage computational tasks, transfer files and access software applications from any location using a web browser. OOD simplifies the process of interacting with HPC systems by offering an intuitive interface, eliminating the need for complex command-line operations, and enabling users to monitor and control their jobs, visualize data and collaborate more effectively."
- name: "operating-system"
  wiki: "Operating_system"
  definition: "OS, is the core software on the computer that manages computing resources, performes installations, and executes available programs. Command-line interface (CLI) and Graphical User Interface (GUI) enable the user directly interact with the operating system to set up, configure, and troubleshoot it. Among the popular operating systems are Windows, Mac OS, and Linux."
# P
- name: "parallel-computing"
  wiki: "Parallel_computing"
  definition: "is a type of computation in which many calculations or processes are carried out simultaneously, leveraging multiple processors or cores to solve complex problems faster. This approach divides a large problem into smaller, independent tasks that can be processed concurrently, significantly reducing computation time. In HPC environments, parallel computing can also involve using modules such as the `parallel` module, which simplifies the execution of parallel tasks for users. This allows even simple tasks to benefit from parallel processing, enhancing performance and efficiency. Parallel computing is essential in fields such as scientific research, engineering and big data analysis, and it is implemented through various models and architectures, including multi-core processors, clusters and distributed systems. It enables the handling of large-scale computations that would be impractical with sequential processing."
- name: "plotting"
  wiki: "Plot_(graphics)"
  definition: "is the process of creating visual representations of data to facilitate understanding and analysis. In the context of research and data science, plotting involves using tools and libraries, such as `Matplotlib`, `Plotly` and `ggplot2`, to generate graphs, charts and plots. These visualizations help in identifying patterns, trends and outliers in the data, making it easier to interpret results and communicate findings effectively."
- name: "programming"
  wiki: "Programming_language"
  definition: "means creating a set of instructions for a computer on how to execute operations in order, following the assumptions and logical conditions of the algorithm. Many programming languages facilitate communication between the code developer and the computing machine. Bash enables a shell scripting using a command-line interpreter for automating repetitive tasks by executing pre-defined commands according to requested conditionals and loops. More advanced analytical operations, including mathematical and statistical functions, modifying complex data structures, or processing non-text data, usually require a higher-level programming language such as Python or C++."
- name: "project-management"
  wiki: "Project_management"
  definition: "is the practice of planning, organizing, and overseeing the execution of a project to achieve specific goals within defined constraints such as time, budget, and resources. It involves coordinating tasks, managing team members, and ensuring that project milestones are met. Key components include defining project objectives, creating detailed plans, allocating resources, monitoring progress, and adjusting strategies as needed to ensure successful project completion and publication of reserch outcomes. Effective project management ensures that projects are delivered on time, within scope, and to the desired quality standards."
- name: "pseudocode"
  wiki: "Pseudocode"
  definition: "is a representation of code or algorithm, summarizing the code. It is a readable step by step description of what the program should do."
# Q
- name: "queue"
  wiki: "Job_queue"
  definition: "in distributed computing, it is an organized list (sequence) of tasks submitted to the job scheduler that manages the computational resources and decides to start or stop the task. The queue is ordered by wait time, user priority, and availability of requested resources. When the combination of these factors is advantageous, the submitted task begins executing, and so its status changes from waiting to running. The queuing system is typical for distributed computing, such as a network of computer clusters shared by more users. Some of the most popular workload managers are SLURM and PBS."
# R
- name: "raw-data"
  wiki: "Raw_data"
  definition: "is the unprocessed data captured directly from its source, retaining its original form without any filtering, cleaning or transformation. It typically has a large volume and includes all the details and potential noise inherent in the data collection process. In data science, raw data serves as the primary input, providing the foundational information required for analysis. Processing raw data involves steps such as cleaning, normalization and transformation to prepare it for meaningful analysis and interpretation. This initial, unaltered state is crucial for ensuring the accuracy and integrity of the subsequent data processing and analysis stages."
- name: "remote-access"
  wiki: "Remote_access"
  definition: "is the ability to connect to and use a computer or network from a different location through the internet or another network. This allows users to access files, applications, and system resources as if they were physically present at the remote location. Tools like SSH (Secure Shell) and VPN (Virtual Private Network) facilitate secure remote access, enabling efficient and flexible work, troubleshooting and system management from anywhere. Open OnDemand (OOD) provides a web-based interface for accessing HPC resources, making it easier for users to manage their computational tasks remotely."
- name: "remote-machine"
  wiki: ""
  definition: "is any other computer or computing network that the user can access by logging into the external network. Performing actions on a remote machine requires a secure login and often requires the user to have an account created by the network administrator. In scientific projects, we use remote computing machines as part of the HPC infrastructure to access high-performance computing and collaboratively share big data."
- name: "resources"
  wiki: "Resource"
  definition: "in the context of research, refer to the various tools, materials, and supports needed to conduct a study or project effectively. This includes physical resources like laboratory equipment, computing resources such as HPC clusters, software and long term storage space, data resources like datasets and databases, and human resources such as skilled personnel and collaborators. Proper management and allocation of these resources are crucial for the success of research activities, ensuring that all necessary components are available and efficiently utilized throughout the research process."
# S
- name: "SLURM"
  wiki: "Slurm_Workload_Manager"
  definition: "(Simple Linux Utility for Resource Management) is a powerful and widely-used cluster management and job scheduling system designed for high-performance computing (HPC) environments. It manages the allocation of resources such as CPU, memory and storage across multiple nodes in a computing cluster. SLURM enables users to submit, schedule, and monitor jobs efficiently, ensuring optimal resource utilization and job prioritization. Its capabilities include workload management, job dependency handling and resource reservation, making it an essential tool for researchers and engineers working in HPC settings. SLURM's flexibility and scalability allow it to support a wide range of computational workloads and complex workflows."
- name: "structured-data"
  wiki: "Data_model"
  definition: "is highly organized in terms of easy digital deciphering. That includes a standardized format, enduring order, and categorization in a well-determined arrangement that facilitates managing and querying datasets in various combinations. A typical example of an organized data structure is a spreadsheet or relational database."
- name: "system-setup"
  wiki: "Operating_system"
  definition: "refers to the process of installing and/or configuring an operating system (OS) on a personal (local) machine, as well as setting up a user account on an HPC cluster (remote machine), including HOME directory management. This involves steps to ensure the command-line interface (CLI) is functional and the installation of useful tools such as office suites, development environments, and graphic software to enhance productivity and reproducibility. Proper system setup maximizes the efficiency and capabilities of the computing tool, allowing users to effectively utilize both local machines and remote HPC resources."
# T
- name: "terminal"
  wiki: "Terminal_emulator"
  definition: "is a program that provides a command-line interface (CLI) for interacting with your computer's operating system. It allows users to access and manage files, execute commands and run scripts by typing text-based commands. The terminal is a powerful tool for performing tasks such as navigating the file system, managing processes, configuring system settings and automating repetitive tasks. It is widely used by developers, system administrators and power users for its efficiency and flexibility compared to graphical user interfaces (GUIs). The terminal is the typical interface for interacting with High-Performance Computing (HPC) systems."
- name: "text-manipulation"
  wiki: "Text_Utilities"
  definition: "is the process of modifying and organizing text to achieve a desired format or structure. In the context of GNU Core Utilities, it involves using command-line tools like `cat`, `cut`, `tr` or `sort` to perform operations such as reading, replacing, extracting and sorting text data. These tools enable efficient and automated handling of text files, simplifying complex text processing tasks."
# U
- name: "unix"
  wiki: "Unix_shell"
  definition: "is a command-line interpreter that provides a user interface for interacting with a computer's operating system (OS). It allows users to execute commands, run scripts and control the execution of programs and procedures through text-based input. Shells such as Bash, Zsh, and Csh interpret and execute user commands, facilitating tasks like file manipulation, process management and system configuration. Shell scripts, which are sequences of commands stored in a file, automate complex tasks and workflows, enhancing productivity and efficiency. "
- name: "unstructured-data"
  wiki: "Unstructured_data"
  definition: "has no organized structure that can be easily detected, processed, and categorized by computer algorithms. This type of data is usually massive and descriptive in nature. A good example is the streams of highly varied text (e.g., emails, social media posts, online blogs, newspapers, books, and scientific publications), audio and video recording, images and photos, data from various sensors (weather, traffic), and medical records."
# V
- name: "version-control"
  wiki: "Version_control"
  definition: "is a system that records changes to files or sets of files over time, allowing users to track revisions, revert to previous versions, and collaborate effectively. In research and software development, version control systems like Git help manage modifications to source code, documents, and datasets, ensuring that every change is documented and can be traced. This enhances collaboration by enabling multiple users to work on the same project simultaneously without conflict and helps maintain a complete history of the project's evolution, aiding in knowledge retention and continuity."
- name: "virtual-environment"
  wiki: "Virtual_environment"
  definition: "is an isolated workspace created to manage and run specific projects without affecting the system's global settings. It allows developers to install and use different versions of software packages and dependencies tailored to a project. Example <a href='https://en.wikipedia.org/wiki/Virtual_environment_software' target='_blank'>virtual environment software</a> includes `virtualenv`, `venv` and `conda` for managing environments and Python packages. These tools help ensure project consistency and prevent conflicts between dependencies in different projects."
- name: "visualization"
  wiki: "Data_and_information_visualization"
  definition: "is a highly visually influential and semantically meaningful form of modern communication methods. In Data Science, interactive graphing and creating concise infographics support both the ease of extracting insights and the opportunity for deeper analysis for those interested. That contributes to better knowledge retention."
# W
- name: "workdir"
  wiki: "Working_directory"
  definition: "is a working directory for a project or computational task. It often appears as a variable or instruction that can be assigned a path to a selected location in the file system. That path is then used for all future commands that require a location, such as writing to a file. It is a common variable for workload managers on distributed computing infrastructures. The pathname of the current working directory can be accessed with the `pwd` command or using the `$PWD` environmental variable."
