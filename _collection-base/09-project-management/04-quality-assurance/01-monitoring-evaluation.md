---
title: "Monitoring & Evaluation"
layout: single
author: Aleksandra Badaczewska
author_profile: true
header:
  overlay_color: "444444"
  overlay_image: 09-project-management/assets/images/09_project_management_banner.png
type: "tutorial"
order: 941
level: 2
categories: ["project-management", "data-management", "code-development", "documentation", "version-control"]
tags: ["standards", "productivity", "reproducibility"]
---


{% include toc %}
{% include images_path %}
{% include page-sourcing.html %}


# Introduction

In this tutorial, let's dive into the world of Monitoring & Evaluation (M&E) and unpack why it's such a game-changer for your research projects. M&E isn't just fancy jargon; **it's a critical toolkit that helps you keep your project on track**, ensuring that you're not just doing work, but you're doing the right work. It's all about making sure that your project hits its marks, delivers on its promises and, most importantly, contributes to knowledge and practice in meaningful ways. By integrating M&E from the get-go, you're setting up your project for success and ensuring that every effort counts towards making a tangible impact.


<div class="protip" markdown="1">
With a little creativity and strategic thinking, Monitoring and Evaluation techniques can reinforce your research and help you **make informed decisions every step of the way**.
</div>

*Let's explore how you can apply M&E principles effectively, ensuring your project is not just a drop in the ocean but a wave of change in your field.*


## Scope and Significance

In small to moderate size research projects, M&E plays a crucial role by providing a structured approach to manage and evaluate the project from inception to conclusion. Its application helps in steering the project towards its defined goals, ensuring resources are used effectively and outcomes are achieved as planned.

<div class="note" markdown="1">
Monitoring & Evaluation (M&E) is a systematic framework designed to assess the progress and effectiveness of a research project's activities, ensuring objectives are efficiently met.
</div>

M&E activities are structured around four pivotal components: `Planning`, `Monitoring`, `Evaluation` and `Adaptation`. Each serves a unique purpose:
- **Planning charting the course for quality oversight.** <br>
<i>This involves outlining specific aspects of the project that will be monitored to ensure standards are met throughout and guarantee the quality of data, methods, and results.</i>

- **Monitoring keeping tabs on your progress.** <br>
<i>Continuously tracking progress against the project plan to ensure activities align with the objectives. It helps in identifying any deviations early on.</i>

- **Evaluation gauging success at key milestones.** <br>
<i>Assessing the project's outcomes against the initial benchmarks and objectives to determine its success and areas for improvement.</i>

- **Adaptation fine-tuning for excellence.** <br>
<i>Implementing changes based on evaluation insights to refine and improve project processes and outcomes. This step involves adjusting strategies, methods, or practices in response to identified challenges and opportunities for enhancement.</i>

<div class="example italic mb-0" markdown="1">
Consider a moderate-scale agricultural study aimed at improving tomato plant yield using two different organic fertilizers.
* **Planning:** The team decides to monitor plant growth rates and fruit yield as key metrics to assess the effectiveness of each fertilizer.
* **Monitoring:** Every two weeks, the team measures the height of tomato plants and records the number of tomatoes produced per plant, comparing these figures against expected growth milestones and yield targets.
* **Evaluation:** After two months, the team evaluates the collected data and discovers that while plants with Fertilizer A are taller, those with Fertilizer B produce more fruit.
* **Adaptation:** Based on these insights, the team adapts their approach by deciding to combine both fertilizers in a new ratio, aiming to optimize both plant height and fruit yield in the next planting cycle.
</div>

<div class="example italic" markdown="1">
Consider a small-scale bioinformatics project focused on identifying genetic markers associated with resistance to a specific antibiotic in bacteria.
<table style="margin-bottom: 0px; margin-top: 10px;">
  <tr><th>Project</th><th>Planning</th><th>Monitoring</th><th>Evaluation</th><th>Adaptation</th></tr>
  <tr>
    <td>Bioinformatics project on antibiotic resistance</td>
    <td>Goal to identify resistance markers via sequence analysis. Key metrics: identification accuracy and computational efficiency.</td>
    <td>Weekly genome analysis to track resistance marker identification against benchmarks.</td>
    <td>Noticed overlooked resistance markers and slower computational processes than expected.</td>
    <td>Refined data algorithms to include more genetic variations and optimized computational methods for speed.</td>
  </tr>
</table>
</div>

## Best Practices

Boosting the quality of research requires a balanced approach that merges quantitative assessments of specific targets in your project, such as measuring data accuracy or computational efficiency, with qualitative actions and a strong commitment to best practices. This holistic approach ensures that while statistics on data provides objective insights into progress and performance, some good habits enrich the research's depth, integrity, and relevance.

<div class="note" markdown="1">
Best practices in M&E emphasize a structured, transparent, and participatory approach to research, ensuring that projects are not only accountable and effective but also adaptable and impactful based on solid evidence and academic community engagement.
</div>

Here are additional activities under Monitoring and Evaluation that can help ensure quality assurance.

### **MONITORING**

<table style="margin: 0;"><tr>
  <th style="width: 25%;">1. Effective Use of Tools</th><td style="border: none;">Select tools that match the projectâ€™s requirements. Proper tools simplify tracking progress and analyzing data, making the M&E process more manageable. Tools should enhance, not complicate the process.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">The project team uses a cloud-based bioinformatics platform (Galaxy) to streamline data analysis. This platform allows for <b>scalable computing resources</b> to handle large genomic datasets efficiently and <b>includes built-in tools</b> for data visualization, which simplifies the monitoring of analysis progress and outcomes. </span>
</div>
</details>

<table style="margin: 0;"><tr>
  <th style="width: 25%;">2. Project Progress Checks</th><td style="border: none;">Conduct regular team meetings to review progress, discuss any issues encountered and ensure that data collection & processing tasks follow the predefined protocols. This communication helps identify and solve issues quickly, ensuring the project stays on course.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">A checklist of key tasks and current status is reviewed periodically, and a chart visualizing trends in quality metrics over time is updated. Additionally, new achievements are marked on a Progress Tracking Board in the lab, providing a clear, visual representation of progress and immediately highlighting any deviations from expected outcomes. </span>
</div>
</details>

<table style="margin: 0;"><tr>
  <th style="width: 25%;">3. Efficient Use of Resources</th><td style="border: none;">Monitor resource allocation, resource usage and work hours spent on each project activity. Keeping a record helps in assessing whether resources are being used efficiently and aids in planning for future needs.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">The team uses a digital tracking system to log hours spent on data analysis and computational resource usage for each phase. Regular reviews of this log help identify areas where resources can be optimized, such as reallocating computational power during off-peak hours to reduce costs, ensuring that the project remains within budget while maximizing productivity. </span>
</div>
</details>

<table style="margin: 0;"><tr>
  <th style="width: 25%;">4. Culture of Accountability</th><td style="border: none;">Assign specific team members the responsibility of performing the accuracy measurements and validating data at each step. This step helps catch mistakes and improves the overall quality of data and methods.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">
One team member is tasked with tracking the computational efficiency of the analysis software, while another is responsible for ensuring data integrity after analysis. They both update their findings in an online-accessible spreadsheet, ensuring a continuous performance overview. This method fosters a culture of accountability and provides real-time access to monitoring outcomes.</span>
</div>
</details>

<table style="margin: 0;"><tr>
  <th style="width: 25%;">5. Transparency <br>in Mid-Project Reporting</th><td style="border: none;">Ensure all aspects of your research, including methodologies, data collection, and findings, are regularly documented and accessible. This fosters trust, supports decision-making and facilitates peer review. It also prepares the ground for future project replication.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">The team sets up a weekly newsletter that details recent activities, <b>preliminary findings, challenges encountered, and strategies employed</b> to overcome them in the ongoing bioinformatics project. This newsletter is shared not only within the team but also with collaborators. It serves as a <b>living documentation</b>, growing with the project, and ensures that when results are ready for publication, the team has a comprehensive and detailed record of the journey, <b>facilitating replication</b> and further research.</span>
</div>
</details>



### **EVALUATION**

<table style="margin: 0;"><tr>
  <th style="width: 25%;">1. Data-Driven Decision Making</th><td style="border: none;">Base decisions on analyzed data rather than assumptions. Making informed decisions ensures project adjustments are grounded in real evidence.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">The project team uses a cloud-based bioinformatics platform (Galaxy) to streamline data analysis. This platform allows for <b>scalable computing resources</b> to handle large genomic datasets efficiently and <b>includes built-in tools</b> for data visualization, which simplifies the monitoring of analysis progress and outcomes. </span>
</div>
</details>

<table style="margin: 0;"><tr>
  <th style="width: 25%;">2. Inclusive  Approaches</th><td style="border: none;">Engage project team and collaborators in the evaluation process to ensure their experiences shape project adaptation. Keep collaborators informed about progress and challenges. This approach enhances project transparency.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;"><b>Monthly virtual meetings</b> are held with all project collaborators, including bioinformatics team, microbiology experts, and funding agency representatives, to review progress, <b>discuss preliminary data</b>, and make collective decisions on any project adjustments, <b>ensuring all voices are heard</b> and considered in steering the project forward. </span>
</div>
</details>

<table style="margin: 0;"><tr>
  <th style="width: 25%;">3. Qualitative Feedback from the Team</th><td style="border: none;">Collect and consider the team's qualitative feedback alongside quantitative data. This feedback ensures diverse perspectives and provides deeper insights into the project's areas for improvement.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">After each analysis phase, feedback sessions <b>gather team observations</b> on software usability, data interpretation, and challenges. These insights are then used to <b>refine analysis protocols</b> and enhance the team's effectiveness, ensuring a more inclusive and comprehensive evaluation of the project's progress.</span>
</div>
</details>

<table style="margin: 0;"><tr>
  <th style="width: 25%;">4. Double-Check and <br>Cross-Validation Mechanisms</th><td style="border: none;">Verify the accuracy of data and tasks by having them reviewed by another team member. The double-check ensures the reliability and accuracy of evaluation results, safeguarding the project's integrity. The peer-review practice ensures high standards are maintained and errors are minimized.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">Each stage of the data analysis process, from raw data processing to final marker identification, is <b>independently reviewed by two team members</b>. For instance, one member performs the initial genomic data alignment, while another independently verifies the alignment accuracy using a subset of the data. This <b>cross-validation approach</b> ensures that computational analyses are robust, <b>reducing the likelihood of errors</b> and increasing confidence in the findings.</span>
</div>
</details>

<table style="margin: 0;"><tr>
  <th style="width: 25%;">5. Continuous Improvement</th><td style="border: none;">Use findings from the evaluation to iteratively enhance project methods and outcomes.  Feedback loops  are crucial for integrating new insights and lessons learned into current and future projects.</td>
</tr></table>

<details style="margin: 0 0 20px 0;"><summary style="background-color: #FFFACD; width: 25%;"><span style="font-weight:800; font-size: 16px;">EXAMPLE:</span></summary>

<div style="background: #FFFACD; padding: 15px; margin-bottom: 20px; font-size: 16px;">
<span style="font-style:italic;">After each phase of genetic marker identification, the team conducted a <b>performance review</b> of the computational models used, comparing predicted resistance patterns with actual laboratory results. This <b>iterative evaluation</b> allowed them to refine their algorithms, improving the accuracy of marker identification in subsequent analyses. </span>
</div>
</details>


### EXCERCISE

<div class="exercise before" markdown="1" data-before="Planning M&E actions in your research">
Choose any current or upcoming project you're involved in. Based on the best practice recommendations provided, draft a brief outline of the Monitoring & Evaluation (M&E) actions you plan to implement. Include specific tools, methods, and frequency of evaluations. <br>
**TIP:** *Ensure your plan addresses each of the key M&E components: Planning, Monitoring, and Evaluation.* <base class="mb">
**Additional Questions to Guide Your Task:**
* What are the primary objectives of your project, and how can they be measured effectively?
* How often will you review and adjust your M&E plan based on findings and feedback?
* Who are the key collaborators in your project, and how will you keep them informed and engaged throughout the M&E process?
* What challenges do you anticipate in implementing your M&E plan, and how might you address them?

<div class="protip font-1 mb-0 before" markdown="1" data-before="PRO TIP">
If you're currently without a project idea, try answering those questions for a hypothetical photogrammetry project focused on surveying corn crops in a region prone to weather hazards, to gain practical insights into planning M&E actions.
</div>
</div>


---

# Tools and Methods

<div class="note" markdown="1">
Selecting the right Monitoring & Evaluation (M&E) tools and methods is pivotal for the success of any research project, especially for those operating on a small to medium scale. **The right set of tools can significantly enhance the effectiveness and efficiency of your project**, enabling you to track progress and make informed decisions with greater precision.
</div>

When it comes to choosing these tools and methods, several key criteria must be considered to ensure they align well with your project's needs. `Cost` is often a primary concern for smaller projects, making affordable or free tools particularly attractive. `Ease of use` is crucial as well, as it impacts the learning curve and the speed at which your team can become productive. `Scalability` ensures that the tool can grow with your project, accommodating more data or users as needed. Lastly, the `relevance` of the tool to your specific project needs is essential to ensure it adds value rather than complexity to your processes.

<table>
  <tr><td>Cost</td><td>Tools should be affordable, with free options or plans that match the project budget.</td></tr>
  <tr><td>Ease of Use</td><td>The learning curve should be minimal, allowing for quick adoption by all team members.</td></tr>
  <tr><td>Scalability</td><td>The tool should accommodate the project's growth, handling more data or users as required.</td></tr>
  <tr><td>Relevance</td><td>It must meet the specific needs of the project, whether for data collection, analysis, or reporting.</td></tr>
</table>

<div class="protip" markdown="1">
Understanding and applying these criteria will help you navigate the wide array of available M&E tools and methods, ensuring you select those that will most effectively support your project's unique requirements.
</div>

## Manual Methods

It's smart to recognize the significance and utility of manual methods and low-tech solutions, especially in scenarios where digital tools may not be feasible or necessary. This can be particularly relevant for small-scale research projects where resources are limited, the datasets are easily manageable or in environments with limited access to technology.

<div class="note" markdown="1">
Manual methods bring several advantages, including simplicity, direct control, and the absence of a steep learning curve associated with many digital tools. Moreover, they **foster a hands-on approach to M&E**, encouraging you to engage closely with the data and processes. This can lead to a deeper understanding of the project's progress and challenges.
</div>

<table>
  <tr>
    <td width=200 style="border: transparent;"><img width="800" src="{{ images_path }}/04_tracking_board.png"></td>
    <td><b>Physical Progress Tracking Boards</b><br>Utilize whiteboards or cork boards to monitor quality assurance milestones and evaluation checkpoints. <br>This visual tool allows teams to mark critical stages in the project's lifecycle, from initial quality benchmarks to subsequent evaluations, facilitating a clear view of progress and areas needing attention.</td>
  </tr>
  <tr>
    <td width=200 style="border: transparent;"><img width="800" src="{{ images_path }}/04_evaluation_form.png"></td>
    <td><b>Paper-based Evaluation Forms</b><br>Design paper forms specifically for evaluation purposes, allowing evaluators to assess project components against predefined quality criteria. <br>These forms can be structured to capture both quantitative data (e.g., ratings on a scale) and qualitative feedback (e.g., open-ended responses), offering a comprehensive view of project performance.</td>
  </tr>
  <tr>
    <td width=200 style="border: transparent;"><img width="800" src="{{ images_path }}/04_quality_metrics.png"></td>
    <td><b>Hand-drawn Charts for Quality Trends</b><br>Manually create charts to visualize trends in quality metrics over time. <br>Drawing these charts can help project teams and collaborators to visually grasp improvements or declines in project quality, enabling targeted discussions on maintaining or enhancing standards.</td>
  </tr>
  <tr>
    <td width=200 style="border: transparent;"><img width="800" src="{{ images_path }}/04_checklist.png"></td>
    <td><b>Checklists for Quality Control Audits</b><br>Develop detailed checklists tailored to the projectâ€™s Quality Assurance standards, to be used during periodic audits or reviews. <br>These checklists ensure that every aspect of the project is evaluated consistently, helping to identify deviations from quality benchmarks and necessary corrective actions.</td>
  </tr>
</table>


## Digital Tools

Digital tools for Monitoring & Evaluation in Quality Assurance have become indispensable in the contemporary research. These tools not only streamline the M&E process but also enhance the accuracy and reliability of your research, offering insights that drive informed decision-making. Below is a selection of digital tools that are particularly useful for M&E tasks in research projects, with a focus on their applicability and ease of use.

**Tools for Evaluating Data and Methods**

| tool category | tool              | easy-to-use level | interface |description |
|---------------|-------------------|-------------------|-----------|-------------|
| Data Validation and Cleaning  | [OpenRefine](https://openrefine.org/) | Medium | desktop app | A powerful tool for working with messy data, cleaning it, transforming it from one format into another, and extending it with web services. |
| Statistical Analysis | [JASP](https://jasp-stats.org/) | High  | desktop app| Facilitates both frequentist and Bayesian analysis, valuable for validating research methods and results. |
| Statistics and Text Mining | [Orange](https://orangedatamining.com/)| High | desktop app | A data mining, machine learning, and data visualization tool that features a visual programming front-end for explorative data analysis and interactive data visualization.|
| Version Control | [Git](https://git-scm.com/) with [GitHub](https://github.com) | Medium | online GUI/CLI | Essential for tracking changes in methodologies and data scripts, enhancing methodological transparency and reproducibility. |
| Validation and Reporting | [Jupyter Notebooks](https://jupyter.org/) | High | UI in web browser | An open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text for transparent reporting. |

<div class="warning" markdown="1">
While digital tools can significantly aid in detecting errors, filling in missing values, and resolving inconsistent formatting, it's crucial to understand that **no single tool can automatically "fix" your data** or provide a one-stop solution for a "beautified" data collection. The landscape of digital tools is vast, and selecting the right one requires careful consideration of your specific project needs and challenges.
</div>


**Tools for Effective Monitoring and Adaptability in Projects**

| tool category | tool              | easy-to-use level | interface |description |
|---------------|-------------------|-------------------|-----------|-------------|
| Decision Support | [KNIME](https://www.knime.com/) | Medium | desktop app | Provides a data analytics platform that supports the integration of various data sources and analysis methods for informed decision-making. |
| Continuous Improvement & Feedback | [SurveyMonkey](https://www.surveymonkey.com/)  | High | online GUI | A tool for creating surveys to collect feedback for continuous project improvements. Offers analytics to make data-driven decisions. |
| Tasks Management     | [Trello](https://trello.com) | High | online GUI | Useful for adapting project plans and tasks based on new decisions, with a visual interface for tracking changes and updates. |
| Collaborative Communication | [Slack](https://slack.com/) | High | desktop app | Facilitates real-time communication and decision-making among project team members, ensuring adaptability and responsiveness to project needs. |
| Survey and Feedback | [Google Forms](https://forms.google.com) | Medium | online GUI | A user-friendly tool for creating surveys, forms, and collecting documents in an organized manner. |
